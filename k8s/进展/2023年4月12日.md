# 集群信息

## 机器列表

| 编号 | name        | 私有ip          | nfsip                   | cpu-mem | vol         | disk     | br      | 目的             | 备注                                 |
| ---- | ----------- | --------------- | ----------------------- | ------- | ----------- | -------- | ------- | ---------------- | ------------------------------------ |
| 1    | m-k8s-c1m1  | 10.160.10.101   | 172.23.16.60            | 6-8     | m-k8s-c1m1  | 10G->40G | k8sc1br | 集群-1/镜像制造2 | 虚拟磁盘10G的原因是base-vol只有10G。 |
| 2    | m-k8s-c1w1  | 10.160.10.111   | -                       | 6-8     | m-k8s-c1w1  | 40G      | k8sc1br | 集群-1           |                                      |
| 3    | m-k8s-c1w2  | 10.160.10.112   | -                       | 6-8     | m-k8s-c1w2  | 40G      | k8sc1br | 集群-1           | CIDR 10.127/16   10.98/12           |
| 4    | m-k8s-c2m1  | 10.160.20.101   | **172.23.16.61**  | 8-16    | m-k8s-c2m1  | 40G      | k8sc2br | 集群-2           |                                      |
| 5    | m-k8s-c2w1  | 10.160.20.111   | -                       | 4-8     | m-k8s-c2w1  | 40G      | k8sc2br | 集群-2           |                                      |
| 6    | m-k8s-c2w2  | 10.160.20.112   | -                       | 8-32    | m-k8s-c2w2  | 40G      | k8sc2br | 集群-2           | CIDR 10.128/16     10.99/12          |
| 7    | m-k8s-min   | 10.63.203.127   | -                       | 6-8     | m-test      | 40G      | lxdbr0  | 最小化集群环境   |                                      |
| 8    | m-k8s-make  | 192.168.122.165 | -                       | 6-8     | m-k8s-make  | 10G->40G | default | 集群基础镜像制造 | 虚拟磁盘10G原因同c1m1 193/16 93/12   |
| 9    | m-k8s-fedc1 | 10.160.100.101  | **172.23.16.121** | 8-16    | m-k8s-fedc1 | 40G      | test    | 集群-test        | （测试集群）180/16   100/12          |
| 10   | m-test-c1w1 | 10.160.100.111  | ``                      | 8-16    | m-test-c1w1 | 40G      | test    | 集群-test        |                                      |
| 11   | m-test-c1w2 | 10.160.100.112  | ``                      | 8-16    | m-test-c1w2 | 40G      | test    | 集群-test        |                                      |
| 12   | m-k8s-web   | 10.160.100.200  | -                       | 8-16    | m-k8s-web   | 40G      | test    | 前端             |                                      |
| 13   | m-k8s-ctl   | 10.160.100.100  | **172.23.16.120** | 8-16    | m-k8s-ctl   | 40G      | test    | 控制器           |                                      |

修改nfsshare为固定ip地址，而不是dhcp

```Bash
network:
  version: 2
  ethernets:
    enp1s0:
      dhcp4: no
      addresses: [10.160.100.100/24]
      gateway4: 10.160.100.1
      nameservers:
        addresses: [223.5.5.5]

      #routes:
      #  - to: 0.0.0.0/0
      #    via: 10.160.10.1
      #    metric: 100
    enp2s0:
      dhcp4: no
      addresses: [172.23.16.120/24]
```

## 本周进展

1. **解决上周出现的集群崩溃问题**

   1. 经过排查，集群崩溃根本原因是 虚拟磁盘空间太小。虚拟机使用Autoinstall Python脚本启动，使用的源镜像vol size只有10G。
   2. 问题背景知识： **节点压力驱逐** ：[节点压力驱逐](https://kubernetes.io/zh-cn/docs/concepts/scheduling-eviction/node-pressure-eviction/) 节点压力驱逐是 kubelet 主动终止 Pod 以回收节点上资源的过程。k8s的每个节点上运行这kubelet组件，kubelet 监控集群节点的**内存、磁盘空间**和文件系统的 inode 等资源。 当这些资源中的一个或者多个达到特定的消耗水平， kubelet 可以主动地使节点上一个或者多个 Pod 失效，以回收资源防止饥饿。kubelet 默认为 每10s评估一次驱逐条件。达到驱逐条件时，会出现类似于MemoryPressure和DiskPressure这样的告警信息。当出现警告不久后 某些Pod 会被驱逐（根据驱逐策略判断Pod是否应该驱逐），甚至节点机器的 **Docker 镜像**也被清理。（http://www.mydlq.club/article/115/  ）
      1. k8s默认的DiskPressure相关的触发条件
         * **memory.available<100Mi:** 当内存下降到 100Mi 时 kubelet 开始驱逐 Pod。
         * **nodefs.available<10%:** 当 kubelet 相关存储可用的存储不足 10% 时开始驱逐 Pod 及其容器来释放磁盘空间。
         * **imagefs.available<15%:** 当容器运行时文件系统可用存储空间不足 15% 时开始驱逐 Pod，并且删除没有被使用的镜像来释放空间。
         * **nodefs.inodesFree<5％:** (仅 Linux 系统): 当容器运行时 inodes 可用存储空间不足 5% 时开始驱逐 Pod 及其容器来释放磁盘空间。
      2. 当触发 DiskPressure 告警时，会产生如下影响:
         * 驱逐 Pod
         * 删除未使用的容器
         * 阻止新创建的 Pod 调度到该节点中
   3. 问题描述：在最小化的k8s环境中运行时不存在问题，最小化的k8s环境只拉取了少量的镜像到本地，所占磁盘空间较小。而在进行service和ingress的实验时，拉取了python镜像、node镜像、nginx镜像等多个镜像，导致磁盘总占用率超过了85%，由此出现了DiskPressure，触发k8s的节点压力驱逐，而我们的这个最小化集群中，所有运行的Pod都是必须存在的组件（例如控制平面组件、网络插件组件、存储插件组件），驱逐这些组件的Pod将导致集群的不可用，从而导致更多的Pod出现error停止工作，甚至无法重启。
   4. 解决方案：
      * 一是提高触发条件。触发条件可以用百分比，也可以使用数值例如10G等。
      * 二是增加资源，例如扩大虚拟机的磁盘空间，扩容并重新分区。
   5. 扩大make的disk。3种方案：

   * 方案1是修改磁盘分区，增大。
   * 方案2是删除c1m1，重新从（已resize镜像vol为40G的）镜像中创建一个新的m-k8s-make。
     基于方案1的实践结果，修改m-k8s-c1m1和m-k8s-make的虚拟磁盘大小。修改为40G，观察是否有问题。

   6. 目前集群运行状态良好。
2. **解决NFS共享网络出现相同IP地址问题。**

   1. 多个机器，不同的mac，在dhcp下仍然具有相同的IP地址，导致k8s集群出现各种意料之外的问题不能完全正常工作（例如CNI插件出现问题，导致master的pod与node上的pod之间通信出现问题。
   2. 解决方案：将NFS的ip地址也改为固定IP地址
3. **解决集群CNI插件Calico问题。**

   1. 问题背景：基于上述NFS问题，当集群中节点存在多网卡时，CNI插件的Calico的BGP协议出现问题。[BGP peer | Calico Documentation](https://docs.tigera.io/calico/next/reference/resources/bgppeer#bgp-peer-definition)
   2. 问题描述：使用kubeadm init 初始化集群，只有master一个节点时，pod-calico-node正常工作，处于ready-1/1和running状态。当向集群中加入其他节点，即，在工作节点上使用kubeadm join加入集群时，master节点的pod-calico-node变为ready-0/1（即其中某个容器down）和running状态，而工作节点上的pod-calico-node处于正常状态。查看master节点的calico-node的日志和告警记录，发现出现下列所示问题

      ```Go
      Warning Unhealthy 30m kubelet Readiness probe failed: 2022-03-30 11:23:59.869 [INFO][208] confd/health.go 180: Number of node(s) with BGP peering established = 0 calico/node is not ready: BIRD is not ready: BGP not established with 192.168.50.42,192.168.50.45,192.168.50.46,192.168.50.47,192.168.50.48
      ```

      同时，该问题将导致无法访问service，例如无法访问prometheus监控系统指标。出现卡顿，pod网络存在问题的。
   3. 解决。

      1. 修改calico的yaml文件

         1. 原因是calico-node这个容器的自动IP检测机制存在问题，多网卡的存在，导致其可能使用enp2s0或者其他异常的IP地址。
         2. 在yaml文件中增加正则表达式，使其使用enp1s0。

         ```YAML
            ···
             nodeAddressAutodetectionV4:
               interface: "enp1.*"  # <-----修改为 算网实际的 集群的 接口
            ···
         ```

         3. 此时，`kubectl delete -f xx.yaml` ，`kubectl create -f xx.yaml`。等待pod运行起来，查看是否有效。
4. 修改代码，增加对service指标的范围查询

   1. 新增函数处理service的范围查询
   2. 增加更多的注释和函数说明
   3. 修改接口
5. **在m-k8s-fedc1测试ingress的可用性**

   1. 进展
      1. ~已经部署service和pod，分别位于na和nb命名空间，其中na可以匹配到，nb无法匹配，验证~~**service是根据pod的label进行匹配Pod**~~的。~（即应用部署代码中至少需要模板map和注入模板函数）
      2. ~已成功部署nginx-ingress并添加规则~
      3. 寻找并部署了两个简单的开源应用进行测试，运行状态良好。

         * 一个是基于vue和python的运维平台
         * 一个是博客系统
      4. 相关文件已上传代码仓库

         ![1681265444203](image/kube-apiserver/1681265444203.png)
6. 应用k8s化、最小化模板

   1. （1.24之前和1.24之后对docker的支持不同，之后的版本不使用docker，而是使用containerd）这里只测试了在k8sv1.22版本中的应用部署。https://kubernetes.io/
   2. 目的是将基于docker和docker-compose的应用，部署方式改为使用k8s部署。寻求算网应用所需最小化模板。
   3. 尝试使用工具 kompose 进行自动转换，如何可以使用则不需要自己编写yaml文件，安装方式如下，改工具不需要kubectl和docker环境

      ```undefined
      curl -L https://github.com/kubernetes/kompose/releases/download/v1.27.0/kompose-linux-amd64 -o kompose
      chmod +x kompose
      sudo mv ./kompose /usr/local/bin/kompose
      ```

      1. 对于简单的docker-compose，工具生成的yaml文件可以直接运行，
      2. 涉及到存储和多pod情况时，可能会出现一些问题，还需要人工修改pvc等信息。
      3. 工具未生成namespaces，且生成的service可能还需要人工修改到nodeport方式。
      4. 工具生成的yaml文件夹杂了过多标签，这些对于目前的应用来说是无用的，增加了yaml文件大小，降低了yaml文件的可读性。
   4. 尝试将内网服务改为使用k8s方式部署

      1. memos  备忘录
      2. homer 主页(配置基于直接映射方式)
      3. vm 密码管理
      4. ~~seafile 多pod-修改复杂~~
   5. [使用 k8s/docker-compose 部署 zookeeper集群](https://www.jianshu.com/p/e0f9bfa6a998)

      比较复杂
   6. [SpringBoot应用部署到K8S上](https://zhuanlan.zhihu.com/p/481520742?utm_id=0)

      过于复杂
   7. [GitHub - openspug/spug: 开源运维平台](https://github.com/openspug/spug)

      1. 完成部署方式转化
   8. 完成初步的最小化模板：已上传至github仓库。（pod，deployment，service，ingress模板）。
      [模板templete](https://github.com/runxinm/k8s-service/tree/main/templete)
      [应用生成:k8s-yaml-gen](https://github.com/runxinm/k8s-service/tree/main/k8s-yaml-gen)

      完成初步的生成应用和部署应用接口

# （研一）k8s学习和go语言学习:

[可参考博客：K8S - 随笔分类](https://www.cnblogs.com/woshinidaye123/category/1906819.html?page=2)

机器  **10.160.100.101**

也可直接查看官方文档对于某一部分的介绍。

  需要了解k8s的

* k8s官方命令行kubectl的常见用法:
  * kubectl  create、apply、get、describe、logs、delete等。
  * ~Kubectl exec、taint等用法~
* node,namespaces,
* pod,deployment,
* service,ingress,
* pv,pvc
* 上述几种资源的yaml文件
* ~污点、容忍性（这部分与k8s调度组件有关系，后续学习，属于进阶实验）~

1. ### pod部署

   1. pod:[k8s核心资源之Pod概念及入门使用讲解&amp;&amp; 资源清单yaml文件内容讲解与编写(三)..._Peter_Hx的博客-CSDN博客](https://blog.csdn.net/qq_39565646/article/details/120279480)
   2. 在默认命名空间，部署一个任意的pod，pod只包含一个容器镜像即可（通过kubectl方式和通过yaml 在kubectl apply两种方式都可）（如下，使用nginx镜像。）
   3. 创建一个新的namespaces，在该namespaces中部署一个任意的pod，只包含一个容器镜像即可
   4. 在新的namespaces中，为这个pod增加标签，key为test，value为s-app
2. ### k8s的pod控制器及基于deployment方式部署pod

   1. [K8S-pod控制器 ](https://www.cnblogs.com/suoluo212/p/17164375.html)
   2. [【Kubernetes】k8s的deployment【控制器】详细说明【pod副本数、HPA、升级镜像】](https://blog.csdn.net/cuichongxin/article/details/120001824?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-120001824-blog-126943869.235^v27^pc_relevant_default&spm=1001.2101.3001.4242.2&utm_relevant_index=4)
   3. 创建一个deployment的yaml文件，可以使用与上述pod相同的image。
   4. 修改deployment的yaml文件，使其管理的副本数为3，之后通过kubectl删除某个对应的pod，观察deployment是否会自动增加新的pod。
3. ### Service

   1. 目标：了解k8s中的几种服务暴露方式
   2. 基于上述的pod或deployment方式部署的pod的label，编写service的yaml文件，使得该service可以根据标签选择到上述pod。service服务暴露方式使用NodePort。
4. ### Ingress

   1. 官网：[Installation Guide - NGINX Ingress Controller](https://kubernetes.github.io/ingress-nginx/deploy/#local-testing)
   2. 明确ingress的作用，以及为什么不使用nodeport（nodeport需要占用集群所有集群的某个端口来暴露服务）
   3. 基于a、b、c，创建两个及以上不同的service。
   4. 配置和使用ingress
   5. 实验
      1. 是否能进行负载均衡
         1. 可以进行七层LB
      2. 增加新的service时，配置是否自动更新（或需要怎么修改service的yaml文件，才可以使得ingress的配置热更新、不需要人工手动改配置文件）
         1. 1-service起别名
         2. ingress添加规则

```YAML
apiVersion: v1
kind: Namespace
metadata:
  name: na
---
apiVersion: v1
kind: Namespace
metadata:
  name: nb

---
apiVersion: v1
kind: Pod
metadata:
  name: my-app1
  namespace: na
  labels:
    app: my-app
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80

---
apiVersion: v1
kind: Pod
metadata:
  name: my-app2
  namespace: na
  labels:
    app: my-app
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80

---
apiVersion: v1
kind: Pod
metadata:
  name: my-app
  namespace: nb
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: my-app1
  namespace: nb
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80
---
apiVersion: v1
kind: Pod
metadata:
  name: my-app2
  namespace: nb
spec:
  containers:
  - name: nginx-container
    image: nginx
    ports:
    - containerPort: 80
```
